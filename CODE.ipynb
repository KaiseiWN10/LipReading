{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19773fb4-605c-41b5-923f-e6fd9967a72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from typing import List\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae5b13ea-18d8-452e-ae49-24005953d64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bc2b986-257c-4495-9700-37d6183462eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(\"abcdefghijklmnopqrstuvwxyz'?!123456789 \")\n",
    "char_to_idx = {c: i for i, c in enumerate(vocab)}\n",
    "idx_to_char = {i: c for c, i in char_to_idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "266d3e05-c190-4350-9b93-7ab646ccf5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_frames(frames):\n",
    "    frames = np.array(frames).astype(np.float32)\n",
    "    mean = frames.mean()\n",
    "    std = frames.std()\n",
    "    return (frames - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b483d552-84a5-408d-a2a5-ccbd2dfa3553",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vid(path: str):\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    frames = []\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        cropped = gray[190:236, 80:220]\n",
    "        frames.append(cropped)\n",
    "    cap.release()\n",
    "    return norm_frames(frames)[..., np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2bb9359-9af2-494d-bc7a-db98ed252d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ali(path: str):\n",
    "    tokens = []\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if parts[2] != 'sil':\n",
    "                tokens.append(' ')\n",
    "                tokens.append(parts[2])\n",
    "    return [char_to_idx[c] for word in tokens for c in word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65cbb374-4b4c-46f3-a19c-4c1bc8417abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LipReadingDataset(Dataset):\n",
    "    def __init__(self, data_dir):\n",
    "        self.video_paths = [os.path.join(data_dir, f) for f in os.listdir(data_dir) if f.endswith('.mpg')]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_path = self.video_paths[idx]\n",
    "        base_name = os.path.splitext(os.path.basename(video_path))[0]\n",
    "        align_path = os.path.join('data', 'alignments', 's1', f'{base_name}.align')\n",
    "\n",
    "        video = load_video(video_path)\n",
    "        alignment = load_alignments(align_path)\n",
    "        return torch.tensor(video).permute(3, 0, 1, 2), torch.tensor(alignment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1d2facb8-9665-4c14-9c4a-d5f89e01b04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(batch):\n",
    "    videos, labels = zip(*batch)\n",
    "    \n",
    "    # Find max sequence length (frames) and label length\n",
    "    max_video_len = max(v.shape[1] for v in videos)\n",
    "    max_label_len = max(l.shape[0] for l in labels)\n",
    "\n",
    "    # Pad videos and labels\n",
    "    padded_videos = []\n",
    "    padded_labels = []\n",
    "    input_lengths = []\n",
    "    label_lengths = []\n",
    "\n",
    "    for v, l in zip(videos, labels):\n",
    "        pad_len = max_video_len - v.shape[1]\n",
    "        padded_v = F.pad(v, (0, 0, 0, 0, 0, 0, 0, pad_len))  # pad time dimension\n",
    "        padded_l = F.pad(l, (0, max_label_len - l.shape[0]), value=0)\n",
    "\n",
    "        padded_videos.append(padded_v)\n",
    "        padded_labels.append(padded_l)\n",
    "        input_lengths.append(v.shape[1])\n",
    "        label_lengths.append(l.shape[0])\n",
    "\n",
    "    return (\n",
    "        torch.stack(padded_videos),\n",
    "        torch.stack(padded_labels),\n",
    "        torch.tensor(input_lengths),\n",
    "        torch.tensor(label_lengths),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a41315ec-c815-40ff-aa79-c02922bdbe10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LipReadingModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super(LipReadingModel2D, self).__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=3, padding=1), \n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2), \n",
    "        )\n",
    "\n",
    "        self.lstm_input_size = 256 * 5 * 17  \n",
    "        self.lstm = nn.LSTM(self.lstm_input_size, 256, num_layers=2, bidirectional=True, batch_first=True)\n",
    "        self.fc = nn.Linear(512, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C, H, W = x.shape\n",
    "\n",
    "        x = x.view(B * T, C, H, W) \n",
    "        x = self.conv(x)         \n",
    "        x = x.view(B, T, -1)     \n",
    "        \n",
    "        x, _ = self.lstm(x)         \n",
    "        x = self.fc(x)           \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "46a17eb0-0c4e-4173-9547-c1ecf9cc0483",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(logits):\n",
    "    pred = torch.argmax(logits, dim=-1)\n",
    "    results = []\n",
    "    for p in pred:\n",
    "        chars = [idx_to_char[idx.item()] for idx in p]\n",
    "        collapsed = []\n",
    "        prev = None\n",
    "        for c in chars:\n",
    "            if c != prev and c != '_':\n",
    "                collapsed.append(c)\n",
    "            prev = c\n",
    "        results.append(''.join(collapsed))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ccbb9da7-2961-432b-b3ee-6ebea251d911",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, opt, criterion, epochs):\n",
    "    model.to(device)\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for videos, labels, input_lengths, label_lengths in train_loader:\n",
    "            videos, labels = videos.to(device), labels.to(device)\n",
    "            opt.zero_grad()\n",
    "            output = model(videos)  # [B, T, V]\n",
    "            output = output.log_softmax(2).permute(1, 0, 2)  # For CTC Loss: [T, B, V]\n",
    "            loss = criterion(output, labels, input_lengths, label_lengths)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for videos, labels, input_lengths, label_lengths in val_loader:\n",
    "                videos = videos.to(device)\n",
    "                output = model(videos)\n",
    "                decoded = greedy_decode(output)\n",
    "                print(\"Predicted:\", decoded[0])\n",
    "                print(\"Actual:\", ''.join([idx_to_char[i.item()] for i in labels[0] if i.item() in idx_to_char]))\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b79d4911-e8e4-4cee-8e96-10e185c3f9fc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "The operator 'aten::max_pool3d_with_indices' is not currently implemented for the MPS device. If you want this op to be considered for addition please comment on https://github.com/pytorch/pytorch/issues/141287 and mention use-case, that resulted in missing op as well as commit hash 134179474539648ba7dee1317959529fbd0e7f89. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m opt = torch.optim.Adam(model.parameters(), lr=\u001b[32m1e-4\u001b[39m)\n\u001b[32m      8\u001b[39m criterion = nn.CTCLoss(blank=\u001b[38;5;28mlen\u001b[39m(vocab))\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(model, train_loader, val_loader, opt, criterion, epochs)\u001b[39m\n\u001b[32m      7\u001b[39m videos, labels = videos.to(device), labels.to(device)\n\u001b[32m      8\u001b[39m opt.zero_grad()\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m output = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideos\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [B, T, V]\u001b[39;00m\n\u001b[32m     10\u001b[39m output = output.log_softmax(\u001b[32m2\u001b[39m).permute(\u001b[32m1\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m2\u001b[39m)  \u001b[38;5;66;03m# For CTC Loss: [T, B, V]\u001b[39;00m\n\u001b[32m     11\u001b[39m loss = criterion(output, labels, input_lengths, label_lengths)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Machine-Learning-Projects/lip_reading/lipread.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Machine-Learning-Projects/lip_reading/lipread.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mLipReadingModel.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     x = F.relu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpool1\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     17\u001b[39m     x = F.relu(\u001b[38;5;28mself\u001b[39m.pool2(\u001b[38;5;28mself\u001b[39m.conv2(x)))\n\u001b[32m     18\u001b[39m     x = F.relu(\u001b[38;5;28mself\u001b[39m.pool3(\u001b[38;5;28mself\u001b[39m.conv3(x)))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Machine-Learning-Projects/lip_reading/lipread.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Machine-Learning-Projects/lip_reading/lipread.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Machine-Learning-Projects/lip_reading/lipread.venv/lib/python3.12/site-packages/torch/nn/modules/pooling.py:296\u001b[39m, in \u001b[36mMaxPool3d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    295\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor):\n\u001b[32m--> \u001b[39m\u001b[32m296\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmax_pool3d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    301\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    302\u001b[39m \u001b[43m        \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    303\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_indices\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreturn_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    304\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Machine-Learning-Projects/lip_reading/lipread.venv/lib/python3.12/site-packages/torch/_jit_internal.py:622\u001b[39m, in \u001b[36mboolean_dispatch.<locals>.fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    620\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m if_true(*args, **kwargs)\n\u001b[32m    621\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m622\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mif_false\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Machine-Learning-Projects/lip_reading/lipread.venv/lib/python3.12/site-packages/torch/nn/functional.py:920\u001b[39m, in \u001b[36m_max_pool3d\u001b[39m\u001b[34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[39m\n\u001b[32m    918\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m stride \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    919\u001b[39m     stride = torch.jit.annotate(\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mint\u001b[39m], [])\n\u001b[32m--> \u001b[39m\u001b[32m920\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmax_pool3d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mNotImplementedError\u001b[39m: The operator 'aten::max_pool3d_with_indices' is not currently implemented for the MPS device. If you want this op to be considered for addition please comment on https://github.com/pytorch/pytorch/issues/141287 and mention use-case, that resulted in missing op as well as commit hash 134179474539648ba7dee1317959529fbd0e7f89. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS."
     ]
    }
   ],
   "source": [
    "train_dataset = LipReadingDataset('./data/s1')\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=collate)\n",
    "\n",
    "val_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=collate)  # Can split for actual val\n",
    "\n",
    "model = LipReadingModel(vocab_size=len(vocab))\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.CTCLoss(blank=len(vocab))\n",
    "\n",
    "train(model, train_loader, val_loader, opt, criterion, 10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lipread.venv",
   "language": "python",
   "name": "lipread.venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
