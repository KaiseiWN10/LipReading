{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19773fb4-605c-41b5-923f-e6fd9967a72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from typing import List\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae5b13ea-18d8-452e-ae49-24005953d64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bc2b986-257c-4495-9700-37d6183462eb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "vocab = list(\"abcdefghijklmnopqrstuvwxyz'?!123456789 \")\n",
    "char_to_idx = {c: i for i, c in enumerate(vocab)}\n",
    "idx_to_char = {i: c for c, i in char_to_idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "266d3e05-c190-4350-9b93-7ab646ccf5cc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def norm_frames(frames):\n",
    "    frames = np.array(frames).astype(np.float32)\n",
    "    mean = frames.mean()\n",
    "    std = frames.std()\n",
    "    return (frames - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b483d552-84a5-408d-a2a5-ccbd2dfa3553",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def load_vid(path: str):\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    frames = []\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        cropped = gray[190:236, 80:220]\n",
    "        frames.append(cropped)\n",
    "    cap.release()\n",
    "    return norm_frames(frames)[..., np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2bb9359-9af2-494d-bc7a-db98ed252d42",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def load_ali(path: str):\n",
    "    tokens = []\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if parts[2] != 'sil':\n",
    "                tokens.append(' ')\n",
    "                tokens.append(parts[2])\n",
    "    return [char_to_idx[c] for word in tokens for c in word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65cbb374-4b4c-46f3-a19c-4c1bc8417abd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class LipReadingDataset(Dataset):\n",
    "    def __init__(self, data_dir):\n",
    "        self.video_paths = [os.path.join(data_dir, f) for f in os.listdir(data_dir) if f.endswith('.mpg')]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_path = self.video_paths[idx]\n",
    "        base_name = os.path.splitext(os.path.basename(video_path))[0]\n",
    "        align_path = os.path.join('data', 'alignments', 's1', f'{base_name}.align')\n",
    "\n",
    "        video = load_video(video_path)\n",
    "        alignment = load_alignments(align_path)\n",
    "        return torch.tensor(video).permute(3, 0, 1, 2), torch.tensor(alignment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1d2facb8-9665-4c14-9c4a-d5f89e01b04e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def collate(batch):\n",
    "    videos, labels = zip(*batch)\n",
    "    \n",
    "    # Find max sequence length (frames) and label length\n",
    "    max_video_len = max(v.shape[1] for v in videos)\n",
    "    max_label_len = max(l.shape[0] for l in labels)\n",
    "\n",
    "    # Pad videos and labels\n",
    "    padded_videos = []\n",
    "    padded_labels = []\n",
    "    input_lengths = []\n",
    "    label_lengths = []\n",
    "\n",
    "    for v, l in zip(videos, labels):\n",
    "        pad_len = max_video_len - v.shape[1]\n",
    "        padded_v = F.pad(v, (0, 0, 0, 0, 0, 0, 0, pad_len))  # pad time dimension\n",
    "        padded_l = F.pad(l, (0, max_label_len - l.shape[0]), value=0)\n",
    "\n",
    "        padded_videos.append(padded_v)\n",
    "        padded_labels.append(padded_l)\n",
    "        input_lengths.append(v.shape[1])\n",
    "        label_lengths.append(l.shape[0])\n",
    "\n",
    "    return (\n",
    "        torch.stack(padded_videos),\n",
    "        torch.stack(padded_labels),\n",
    "        torch.tensor(input_lengths),\n",
    "        torch.tensor(label_lengths),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a41315ec-c815-40ff-aa79-c02922bdbe10",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LipReadingModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super(LipReadingModel2D, self).__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=3, padding=1), \n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2), \n",
    "        )\n",
    "\n",
    "        self.lstm_input_size = 256 * 5 * 17  \n",
    "        self.lstm = nn.LSTM(self.lstm_input_size, 256, num_layers=2, bidirectional=True, batch_first=True)\n",
    "        self.fc = nn.Linear(512, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C, H, W = x.shape\n",
    "\n",
    "        x = x.view(B * T, C, H, W) \n",
    "        x = self.conv(x)         \n",
    "        x = x.view(B, T, -1)     \n",
    "        \n",
    "        x, _ = self.lstm(x)         \n",
    "        x = self.fc(x)           \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "46a17eb0-0c4e-4173-9547-c1ecf9cc0483",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def decode(logits):\n",
    "    pred = torch.argmax(logits, dim=-1)\n",
    "    results = []\n",
    "    for p in pred:\n",
    "        chars = [idx_to_char[idx.item()] for idx in p]\n",
    "        collapsed = []\n",
    "        prev = None\n",
    "        for c in chars:\n",
    "            if c != prev and c != '_':\n",
    "                collapsed.append(c)\n",
    "            prev = c\n",
    "        results.append(''.join(collapsed))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ccbb9da7-2961-432b-b3ee-6ebea251d911",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, opt, criterion, epochs):\n",
    "    model.to(device)\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for videos, labels, input_lengths, label_lengths in train_loader:\n",
    "            videos, labels = videos.to(device), labels.to(device)\n",
    "            opt.zero_grad()\n",
    "            output = model(videos)  # [B, T, V]\n",
    "            output = output.log_softmax(2).permute(1, 0, 2)  # For CTC Loss: [T, B, V]\n",
    "            loss = criterion(output, labels, input_lengths, label_lengths)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for videos, labels, input_lengths, label_lengths in val_loader:\n",
    "                videos = videos.to(device)\n",
    "                output = model(videos)\n",
    "                decoded = greedy_decode(output)\n",
    "                print(\"Predicted:\", decoded[0])\n",
    "                print(\"Actual:\", ''.join([idx_to_char[i.item()] for i in labels[0] if i.item() in idx_to_char]))\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5466942a-a7e4-4c05-9dd9-7d1e82fe02be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    def __init__(self, video_dir, alignment_dir, vocab):\n",
    "        self.video_paths = []\n",
    "        self.align_paths = []\n",
    "        self.vocab = vocab\n",
    "\n",
    "        for file in os.listdir(video_dir):\n",
    "            if file.endswith('.mpg'):\n",
    "                self.video_paths.append(os.path.join(video_dir, file))\n",
    "                align_file = file.replace('.mpg', '.align')\n",
    "                self.align_paths.append(os.path.join(alignment_dir, align_file))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_path = self.video_paths[idx]\n",
    "        align_path = self.align_paths[idx]\n",
    "\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frames = []\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            gray = cv2.resize(gray, (140, 46))\n",
    "            frames.append(gray)\n",
    "        cap.release()\n",
    "\n",
    "        frames = np.stack(frames)\n",
    "        frames = torch.tensor(frames, dtype=torch.float32) / 255.0\n",
    "        frames = frames.unsqueeze(1)\n",
    "\n",
    "        with open(align_path, 'r') as f:\n",
    "            words = []\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) == 3 and parts[2] not in ['sil', 'sp']:\n",
    "                    words.append(parts[2])\n",
    "        \n",
    "        label = [self.vocab[c] for word in words for c in word]\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "        return frames, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fb9c226-1136-4595-b09b-f98df27e0e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    videos, labels = zip(*batch)\n",
    "\n",
    "    max_len = max(v.size(0) for v in videos)\n",
    "    padded_videos = []\n",
    "    for v in videos:\n",
    "        pad = torch.zeros(max_len - v.size(0), *v.shape[1:])\n",
    "        padded = torch.cat([v, pad], dim=0)\n",
    "        padded_videos.append(padded)\n",
    "    videos = torch.stack(padded_videos)\n",
    "\n",
    "    label_lengths = torch.tensor([len(l) for l in labels], dtype=torch.long)\n",
    "    labels = torch.cat(labels)\n",
    "    input_lengths = torch.full((len(videos),), fill_value=max_len, dtype=torch.long)\n",
    "\n",
    "    return videos, labels, input_lengths, label_lengths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f63aa803-b340-4d44-bd3a-58ccb33967f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LipReadingModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2), \n",
    "\n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  \n",
    "\n",
    "            nn.Conv2d(128, 256, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "        self.lstm = nn.LSTM(256 * 5 * 17, 256, num_layers=2, bidirectional=True, batch_first=True)\n",
    "        self.fc = nn.Linear(512, vocab_size)\n",
    "\n",
    "    def forward(self, x): \n",
    "        B, T, C, H, W = x.shape\n",
    "        x = x.view(B * T, C, H, W)\n",
    "        x = self.conv(x)      \n",
    "        x = x.view(B, T, -1)    \n",
    "        x, _ = self.lstm(x)         \n",
    "        x = self.fc(x)               \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9897c75-b2cf-4e0f-a589-025d87001c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, criterion, epochs, device):\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for videos, labels, input_lengths, label_lengths in train_loader:\n",
    "            videos = videos.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(videos)                         # (B, T, V)\n",
    "            outputs = F.log_softmax(outputs, dim=2).permute(1, 0, 2)  # (T, B, V)\n",
    "\n",
    "            loss = criterion(\n",
    "            outputs.cpu(),\n",
    "            labels.cpu(),\n",
    "            input_lengths.cpu(),\n",
    "            label_lengths.cpu()\n",
    "            )\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38626633-466f-4ef1-8982-427bd31f40ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "vocab_chars = sorted(set(\"abcdefghijklmnopqrstuvwxyz \")) \n",
    "vocab = {c: i for i, c in enumerate(vocab_chars)}\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "dataset = Dataset(\n",
    "    video_dir=\"./data/s1\",\n",
    "    alignment_dir=\"./data/alignments/s1\",\n",
    "    vocab=vocab\n",
    ")\n",
    "loader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model = LipReadingModel(vocab_size)\n",
    "criterion = nn.CTCLoss(blank=vocab_size - 1)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "train(model, loader, optimizer, criterion, epochs=30, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5ef329a-1c55-40ae-84f4-3b3d1139235e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'saved_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa37b797-681a-4238-b462-335f01195c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_chars = sorted(set(\"abcdefghijklmnopqrstuvwxyz \")) \n",
    "vocab = {c: i for i, c in enumerate(vocab_chars)}\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "dataset = Dataset(\n",
    "    video_dir=\"./data/s1\",\n",
    "    alignment_dir=\"./data/alignments/s1\",\n",
    "    vocab=vocab\n",
    ")\n",
    "loader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9de2dd73-7e5c-4124-85cb-ab58837c338b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LipReadingModel(\n",
       "  (conv): Sequential(\n",
       "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): ReLU()\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU()\n",
       "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (lstm): LSTM(21760, 256, num_layers=2, batch_first=True, bidirectional=True)\n",
       "  (fc): Linear(in_features=512, out_features=27, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LipReadingModel(vocab_size)\n",
    "model.load_state_dict(torch.load('saved_model.pth'))\n",
    "model.to(device)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "411d3e5c-0159-4f8a-91aa-41039e072dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TESTING MODEL\n",
    "def greedy_decode(output, blank_idx):\n",
    "    # output: (T, B, V)\n",
    "    output = output.permute(1, 0, 2)  # (B, T, V)\n",
    "    pred_sequences = []\n",
    "    \n",
    "    for batch in output:\n",
    "        pred = torch.argmax(batch, dim=1).cpu().numpy()\n",
    "        prev = -1\n",
    "        decoded = []\n",
    "        for p in pred:\n",
    "            if p != prev and p != blank_idx:\n",
    "                decoded.append(p)\n",
    "            prev = p\n",
    "        pred_sequences.append(decoded)\n",
    "    return pred_sequences\n",
    "def indices_to_text(indices, inv_vocab):\n",
    "    return ''.join([inv_vocab[i] for i in indices])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "155b20b9-454b-498c-9aab-ae6c297e2e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_vocab = {i: c for c, i in vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5546b9b-ee39-4ab3-9e47-d5e15451e1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    sample, label = dataset[0]\n",
    "    input_len = torch.tensor([sample.shape[0]])\n",
    "    label_len = torch.tensor([label.shape[0]])\n",
    "\n",
    "    sample = sample.unsqueeze(0).to(device)\n",
    "    output = model(sample) \n",
    "    output = F.log_softmax(output, dim=2).permute(1, 0, 2)  \n",
    "\n",
    "    pred_indices = greedy_decode(output, blank_idx=vocab_size - 1)[0]\n",
    "    pred_text = indices_to_text(pred_indices, inv_vocab)\n",
    "\n",
    "    true_text = indices_to_text(label.tolist(), inv_vocab)\n",
    "\n",
    "    print(f\"\\nPredicted: {pred_text}\")\n",
    "    print(f\"Ground Truth: {true_text}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lipread.venv",
   "language": "python",
   "name": "lipread.venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
